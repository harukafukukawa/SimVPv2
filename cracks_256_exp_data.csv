device,dist,display_step,res_dir,ex_name,use_gpu,gpu,seed,fps,resume_from,auto_resume,test,batch_size,val_batch_size,num_workers,data_root,dataname,pre_seq_length,aft_seq_length,total_length,method,config_file,model_type,drop,drop_path,epoch,log_step,opt,opt_eps,opt_betas,momentum,weight_decay,sched,lr,lr_k_decay,warmup_lr,min_lr,final_div_factor,warmup_epoch,decay_epoch,decay_rate,filter_bias_and_bn,spatio_kernel_enc,spatio_kernel_dec,hid_S,hid_T,N_T,N_S,user,error,reverse_scheduled_sampling,r_sampling_step_1,r_sampling_step_2,r_exp_alpha,scheduled_sampling,sampling_stop_iter,sampling_start_value,sampling_changing_rate,num_hidden,filter_size,stride,patch_size,layer_norm,train_time,test_time,mse,mae,fid,prev_frame_mse,ssim,psnr
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,512.0,8.0,4.0,haruka,name 'Cracks' is not defined,,,,,,,,,,,,,,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_ConvLSTM,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,ConvLSTM,./configs/cracks_256/ConvLSTM.py,,2.0,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.0001,0.001,0.5,0.0001,10000.0,5,100,0.1,False,,,,,,,haruka,name 'Cracks' is not defined,0.0,25000.0,50000.0,5000.0,1.0,50000.0,1.0,2e-05,"256,256,256,256",5.0,1.0,2.0,0.0,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,512.0,8.0,4.0,haruka,CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.66 GiB total capacity; 19.05 GiB already allocated; 374.00 MiB free; 19.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,,,,,,,,,,,,,,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_PredRNN,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,PredRNN,./configs/cracks_256/PredRNN.py,,1.0,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.0001,2.0,0.1,1e-06,10000.0,5,100,0.1,False,,,,,,,haruka,CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.66 GiB total capacity; 19.27 GiB already allocated; 185.88 MiB free; 19.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,0.0,25000.0,50000.0,5000.0,1.0,50000.0,1.0,2e-05,"256,256,256,256",5.0,1.0,2.0,0.0,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,512.0,8.0,4.0,haruka,CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.66 GiB total capacity; 19.68 GiB already allocated; 141.00 MiB free; 19.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,,,,,,,,,,,,,,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,256.0,8.0,4.0,haruka,Unable to find a valid cuDNN algorithm to run convolution,,,,,,,,,,,,,,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,256.0,8.0,4.0,haruka,Unable to find a valid cuDNN algorithm to run convolution,,,,,,,,,,,,,,,,,,,,,
cuda,False,10,./results,cracks_jun_bin_256_SimVP,True,0,42,False,,False,False,8,4,8,./data/,cracks_256,,,,SimVP,./configs/cracks_256/SimVP.py,,0.001,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.01,0.01,0.01,0.0001,10000.0,5,100,0.1,False,3.0,3.0,64.0,512.0,4.0,4.0,haruka,,,,,,,,,,,,,,,25304.58452296257,1.9073486328125e-05,529.2589111328125,1089.5040283203125,,130.2222900390625,-0.0174637429474589,46.58290111626706
cuda,False,10,./results,cracks_jun_bin_256_ConvLSTM,True,0,42,False,,True,False,8,4,8,./data/,cracks_256,,,,ConvLSTM,./configs/cracks/ConvLSTM.py,,2.0,0.1,250,1,rmsproptf,,,0.9,1e-05,onecycle,0.0001,0.001,0.5,0.0001,10000.0,5,100,0.1,False,,,,,,,haruka,"Error(s) in loading state_dict for ConvLSTM_Model:
	size mismatch for cell_list.0.conv_x.0.weight: copying a param with shape torch.Size([1024, 4, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 4, 5, 5]).
	size mismatch for cell_list.0.conv_h.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.0.conv_o.0.weight: copying a param with shape torch.Size([256, 512, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 256, 5, 5]).
	size mismatch for cell_list.0.conv_last.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).
	size mismatch for cell_list.1.conv_x.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.1.conv_h.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.1.conv_o.0.weight: copying a param with shape torch.Size([256, 512, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 256, 5, 5]).
	size mismatch for cell_list.1.conv_last.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).
	size mismatch for cell_list.2.conv_x.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.2.conv_h.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.2.conv_o.0.weight: copying a param with shape torch.Size([256, 512, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 256, 5, 5]).
	size mismatch for cell_list.2.conv_last.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).
	size mismatch for cell_list.3.conv_x.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.3.conv_h.0.weight: copying a param with shape torch.Size([1024, 256, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 128, 5, 5]).
	size mismatch for cell_list.3.conv_o.0.weight: copying a param with shape torch.Size([256, 512, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 256, 5, 5]).
	size mismatch for cell_list.3.conv_last.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).
	size mismatch for conv_last.weight: copying a param with shape torch.Size([4, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 128, 1, 1]).",0.0,25000.0,50000.0,5000.0,1.0,50000.0,1.0,2e-05,"128,128,128,128",5.0,1.0,2.0,0.0,,,,,,,,
